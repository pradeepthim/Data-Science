---
title: "R Notebook"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


## cross validation is a way to estimate the generalization error. 
##In this exercise we will implement the cross validation method from scratch.
## We want to perform a regression of the form Y ∼ Σ χk (the summation is from k= 1 to d)  and use cross validation as a method to identify the optimal d. 


## We first construct our dataset in the following way: -
## Sample a vector X ∈ R^n where each Xi ∈ U[0, 1]. Each sample point Xi is sampled from the uniform distribution. Construct Y from X using the following equation

##Y = 3X⁵  + 2X² + ϵ

##where ϵ ∈ R^n
##Each ϵi is sampled independently from the N(0,0.5) (normal distribution).
##Choose n = 10000.


```{r}
set.seed(1234)  ## to make sure our randomly selected errors are replicated throughout the code

X <- runif(10000,0,1)

Y <-  (3 * X^5) + (2 * X^2) + (rnorm(10000, 0, 0.5))

data <- data.frame(X,Y) 
```


##Split the 10000 points into a 80% training and 20% test split. 

```{r}
# download required data and packages

library(caTools)
library(glmnet)
library(dplyr)

# Use a seed before randomizing to replicate results.
set.seed(1234)

data$id <- 1:nrow(data)

# Split the data into train(80% of data) and test (20% of data)

train_data <- data %>% dplyr::sample_frac(0.8)
test_data <- dplyr::anti_join(data, train_data, by='id')

dim(train_data)
dim(test_data)

train_data2 <- train_data[,1:2]
test_data2 <- test_data[,1:2]
```

## Split the training set into 5 parts and use the five folds to choose the
## optimal d. The loss function we would implement is the MSE error.
## You want to estimate the MSE error on each fold for a model that has
## been trained on the remaining 4 folds. The cross validation (CV) error
## for the training set would be the average MSE across all five folds. 

```{r}
#number of folds
k <- 5

#  values of d
d_vals <- 1:10

# Split the train data into 5 folds
folds <- split(train_data2, rep(1:k, length.out=nrow(train_data2)))
#class(folds)

# vector to store the CV errors for each value of d
cv_errors <- numeric(length(d_vals))

# For loop for each value of d
for (i in 1:length(d_vals)) {
  
  d <- d_vals[i]  # Get the current value of d
  mse_errors <- numeric(k)   # Initialize a vector to store the MSE errors for each fold
  
  # For loop for each of 5 folds
  for (j in 1:k) {
    
    val_data <- folds[[j]] # Get the jth fold as the validation set
    train_folds <- folds[-j] # Get the remaining folds as the training set
    train_data3 <- do.call(rbind, train_folds)
    model3 <- lm(Y ~ poly(X, d), data=list(train_data3)) #Fit a polynomial reg model with current value of d
    preds3 <- predict(model3, newdata=val_data) # Calculate the MSE error on the validation set
    mse_errors[j] <- mean((val_data$Y - preds3)^2)
  }
  
  cv_errors[i] <- mean(mse_errors) # Calculate the avg MSE error across all folds
}

cv_errors

```

## Plot the CV error as a function of d for d ∈ [1, 2, . . . , 10]

```{r}
# Plot the CV error as a function of d
plot(d_vals, cv_errors, type="l", xlab="d", ylab="CV MSE")


sprintf("From the graph, as the value of d increases, the average MSE from cv is decreasing. Fron d=4, the avg MSE is more or less constant.")
```

## Use the entire training set for training the models.
##Compute the performance of the 10 models on the test set. Plot the test MSE and training MSE as a function of d. Comment on your observations

```{r}
train_mse4 <- numeric(10)
test_mse4 <- numeric(10)

for (i in 1:length(d_vals)){
  d4 <- d_vals[i]
  model4 <- lm(Y~poly(X,d4), data=train_data2)
  
  train_pred4 <- predict(model4, train_data2)
  train_mse4[i] <-  mean((train_pred4 - train_data2$Y)^2)
  
  test_pred4 <- predict(model4, test_data2)
  test_mse4[i] <-  mean((test_pred4 - test_data2$Y)^2)
}

plot(d_vals, train_mse4, type = "l", xlab = "d", ylab = "MSE", main = "Train MSE vs d", col="green")
lines(d_vals, test_mse4, type = "l", col = "red", 
      xlab = "d", ylab = "MSE", main = "Test MSE vs d")
legend("topleft", c("Train MSE", "Test MSE"), lty = c(1, 1), col = c("green", "red"))


sprintf("The MSE for train dataset which is considered in sample is lower as compared to MSE for test dataset for lower values of d. As the values of d increase, that is as the functional form has started becoming higher degree polynomial, the MSE for train and test data is almost same. Since we already know the original functional form, it only makes sense for the MSE to reduce overall and difference in MSE for train and test is converging. Also, the MSE is not zero given that the original functional form is combination of 2 x's with degrees 2 and 5 and also there is an error term which is normally distributed.")

```

